---
title: "Project"
author: "Isaac Attuah"
date: "Saturday, February 19th 2022"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

## Preparing Notebook

```{r}
# Clear workspace
rm(list = ls())

# Set Working Directory
# setwd("./assignment_one")
# getwd()

# Load Libraries
library(ISLR2)
library(ggcorrplot)
library(tidyverse)
library(MASS)
library(boot)
library(tree)
library(randomForest)

# Load Dataset
onset_data <- read.csv("onset.csv")
extra_data <- read.csv("armed.csv")

# Merge Datasets
war_data = merge(onset_data, extra_data, by = c("year", "gwno_a"))

# shuffle the dataframe by rows
# war_data <- war_data[sample(1:nrow(war_data)), ]

# Turn multiple columns to factor
cols <- c("gwno_a", "newconf", "onset1", "onset2", "onset3", "onset5", "onset10", "onset20", "intensity_level", "cumulative_intensity","ep_end")
war_data[cols] <- lapply(war_data[cols], as.factor) 

# Clean and Attach Data
# war_data <- war_data[!names(war_data) %in% c("year_prev")]

war_data <- na.omit(war_data)

#attach(war_data)
# wd_merge <- merge(x=war_data, y=extra, by="gwno_a")
# war_data <- wd_merge
# View Data
head(war_data, 10)
```

## Dataset Information

### General Information

```{r}
message('Dimensions of Dataset')
dim(war_data)
message("Number of Rows ", nrow(war_data))
message("Number of Columns ", ncol(war_data))
```

### Data Summary

```{r}
summary(war_data)
```

## Correlation

Since the values are categorical, we will resort to using

```{r}
war_data_cor <- war_data[ , !names(war_data) %in% c("abc","name", "year", "gwno_a", "onset1")]

library(ggcorrplot)
model.matrix(~0+., data=war_data_cor) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```

## Dividing Into Training and Testing

```{r}
# Divide data into training and testing (# 3)
# Examples for training = (0.80 * 683) = 546 entries
# Examples for testing test = (0.20 * 683) = 137 entries

set.seed(222)
sample_size = round(nrow(war_data)*.80) # setting sample size is 80%
index <- sample(seq_len(nrow(war_data)), size = sample_size)

train_better <- war_data[index, ]
test_better <- war_data[-index, ]

message("Number of Training Examples: ", nrow(train_better))
message("Number of Testing Examples: ", nrow(test_better))
# train_better
# test_better

train_valid <- train_better[ , !names(train_better) %in% c("abc","name", "year", "gwno_a", "onset1", "year_prev")]

test_valid <- test_better[ , !names(test_better) %in% c("abc","name", "year", "gwno_a", "onset1", "year_prev")]

war_data_valid <- war_data[ , !names(test_better) %in% c("abc","name", "year", "gwno_a", "onset1")]
```

# **Phase 1: Predicting War Outcome In 20 Years**Â 

## Model Testing

### Logistic Regression Model

```{r}
# Making model with all input variables

#There is not enough variation in onset1 so we will not include in the regression
glm.fits = glm(onset20 ~ newconf+onset2+onset3+onset5+onset10+duration+year_prev+duration+incompatibility+intensity_level+cumulative_intensity+ep_end,
               data = train_better, family = binomial)

# glm.fits = glm(duration ~ onset2+onset3+onset5+onset10+onset20, data = train_better)

summary(glm.fits)
```

```{r}
# Make predictions based on model
glm.probs = predict(glm.fits,test_better, type="response")

# Initialize vector with 109 elements
glm.pred = rep(0, nrow(test_better))
# Assign 1 to probabilities > 0.5
glm.pred[glm.probs >.5]=1

message('0 for no conflict, 1 for new conflict')
message('Confusion Matrix')

# Confusion Matrix
table(glm.pred,test_better$onset20)

# Test Error
message('Test Error Rate')
mean(glm.pred!=test_better$onset20)
```

### LDA Model

```{r}

# Making model with all input variables
lda.fit=lda(onset20 ~ newconf+onset2+onset3+onset5+onset10+duration+year_prev+duration+incompatibility+intensity_level+cumulative_intensity+ep_end,
               data = train_better)

lda.fit

summary(lda.fit)
lda.pred <- predict(lda.fit , test_better)

message('2 for benign, 4 for malignant')
message('Confusion Matrix')
# Confusion Matrix
lda.class <- lda.pred$class
table(lda.class, test_better$onset20)

message('Test Error Rate')
# Test Error
mean(lda.class != test_better$onset20)
```

#### Linear Discriminants

```{r}
plot(lda.fit)
```

### Decision Trees (Generic)

```{r}
tree.onset20=tree(onset20 ~ newconf+onset2+onset3+onset5+onset10+duration+year_prev+duration+incompatibility+intensity_level+cumulative_intensity+ep_end , war_data_valid)
summary(tree.onset20)
plot(tree.onset20)
text(tree.onset20, pretty = 0, cex=0.75)
```

### Decision Trees (With Training & Testing)

```{r}
# Train using training set
tree.onset20=tree(onset20 ~ newconf+onset2+onset3+onset5+onset10+duration+incompatibility+intensity_level+cumulative_intensity+ep_end , train_valid)

# Test on test set using predict()
# type="class" to return the class prediction
tree.pred=predict(tree.onset20,test_valid,type="class")

# Confusion matrix
conf.matrix <- table(tree.pred,test_valid$onset20)
conf.matrix

# Accuracy on test set
(conf.matrix[1,1] + conf.matrix[2,2])/(conf.matrix[1,1] + conf.matrix[2,2] + conf.matrix[1,2]+ conf.matrix[2,1])
```

### Regression Trees

```{r}

set.seed(1)

tree.onset20=tree(onset20 ~ newconf+onset2+onset3+onset5+onset10+duration+incompatibility+intensity_level+cumulative_intensity+ep_end  , train_valid)
# Only a few of the variables were used in constructing the tree
# lstat: percentage of individuals with lower socioeconomic status
summary(tree.onset20)

# Plot the tree
# Lower values of lstat correspond to more expensive houses
plot(tree.onset20)
text(tree.onset20,pretty=0,cex=0.75)

# cv.tree() to determine whether pruning improves performance
cv.onset20=cv.tree(tree.onset20)
# It doesn't seem to be the case
plot(cv.onset20$size,cv.onset20$dev,type="b")

# prune.tree(): function to prune to be used in case we wanted to prune the tree
prune.onset20=prune.tree(tree.onset20,best=5)
plot(prune.onset20)
text(prune.onset20,pretty=0,cex=0.75)

# Predicting based on CV results (i.e., use the unpruned tree)
yhat=predict(tree.onset20,newdata=test_valid)

# plot(yhat,test_valid$onset20)
# abline(0,1)
# Test error
mse=mean((yhat-test_valid$onset20)^2)
mse
# This model leads to test predictions that are within around $5-6K of the true
# median home value for the suburb
sqrt(mse)
```

### Random Forests

```{r}
# By default randomForest() uses m=p/3 for regression and m=sqrt(p) for classification
# Let's try m=6
set.seed(1)
rf=randomForest(onset20 ~ newconf+onset2+onset3+onset5+onset10+duration+incompatibility+intensity_level+cumulative_intensity+ep_end,data=train_valid,mtry=5,importance =T)
yhat.rf = predict(tree.onset20,newdata=test_valid)

mean((yhat.rf-as.integer(test_valid$onset20))^2)

# importance(): view the importance of each variable
# %IncMSE: mean decrease of accuracy in predictions on the OOB samples when a 
# given variable is excluded from the model
# IncNodeImpurity: total decrease in node impurity that results from splits over
# that variable, averaged over all trees (RSS in regr. vs. deviance in class.)
importance(rf)

# varImpPlot(): Variance importance plot
varImpPlot(rf)
```

### Other Models

-   Penalized Logistic Regression -`plr`

-   Conditional Inference Random Forest -`cforest`

-   Random Forest - `rf`

-   Bayesian Generalized Linear Model -`bayesglm`

-   Boosted Generalized Additive Model **-** `gamboost`

-   Support Vector Machines with Linear Kernel - `svmLinear`

```{r}
library(caret)

#specify the cross-validation method
ctrl <- trainControl(method = "cv")

#fit a regression model and use LOOCV to evaluate performance
model <- train(onset20~newconf+onset2+onset3+onset5+onset10, data = train_better, method = "knn", trControl = ctrl)

#view summary of LOOCV               
print(model)
```

```{r}
predictions <- predict(model, test_better, type="raw")

message('0 for no conflict, 1 for new conflict')
message('Confusion Matrix')

# Confusion Matrix
table(predictions,test_better$onset20)

# Test Error
message('Test Error Rate')
mean(predictions!=test_better$onset20)
```

```{r}
confusionMatrix(data = predict(model, test_better), test_better$onset20)
```

# **Phase 2: Predicting the End Of War**

### Logistic Regression Model

```{r}
# Making model with all input variables

#There is not enough variation in onset1 so we will not include in the regression
glm.fits = glm(ep_end ~ newconf+onset2+onset3+onset5+onset10+duration+year_prev+duration+incompatibility+intensity_level+cumulative_intensity+onset20,
               data = train_better, family = binomial)

# glm.fits = glm(duration ~ onset2+onset3+onset5+onset10+onset20, data = train_better)

summary(glm.fits)
```

```{r}
# Make predictions based on model
glm.probs = predict(glm.fits,test_better, type="response")

# Initialize vector with 109 elements
glm.pred = rep(0, nrow(test_better))
# Assign 1 to probabilities > 0.5
glm.pred[glm.probs >.5]=1

message('0 for no conflict, 1 for new conflict')
message('Confusion Matrix')

# Confusion Matrix
table(glm.pred,test_better$ep_end)

# Test Error
message('Test Error Rate')
mean(glm.pred!=test_better$ep_end)
```

### LDA Model

```{r}

# Making model with all input variables
lda.fit=lda(ep_end ~ newconf+onset2+onset3+onset5+onset10+duration+year_prev+duration+incompatibility+intensity_level+cumulative_intensity+onset20,
               data = train_better)

lda.fit

summary(lda.fit)
lda.pred <- predict(lda.fit , test_better)

message('2 for benign, 4 for malignant')
message('Confusion Matrix')
# Confusion Matrix
lda.class <- lda.pred$class
table(lda.class, test_better$onset20)

message('Test Error Rate')
# Test Error
mean(lda.class != test_better$onset20)
```

#### Linear Discriminants

```{r}
plot(lda.fit)
```

### Decision Trees (Generic)

```{r}
tree.ep_end=tree(ep_end ~ newconf+onset2+onset3+onset5+onset10+duration+year_prev+duration+incompatibility+intensity_level+cumulative_intensity+ep_end+onset20 , war_data_valid)
summary(tree.ep_end)
plot(tree.ep_end)
text(tree.onset20, pretty = 0, cex=0.75)
```

### Decision Trees (With Training & Testing)

```{r}
# Train using training set
tree.ep_end=tree(ep_end ~ newconf+onset2+onset3+onset5+onset10+duration+incompatibility+intensity_level+cumulative_intensity+onset20 , train_valid)

# Test on test set using predict()
# type="class" to return the class prediction
tree.ep_end=predict(tree.onset20,test_valid,type="class")

# Confusion matrix
conf.matrix <- table(tree.ep_end,test_valid$ep_end)
conf.matrix

# Accuracy on test set
(conf.matrix[1,1] + conf.matrix[2,2])/(conf.matrix[1,1] + conf.matrix[2,2] + conf.matrix[1,2]+ conf.matrix[2,1])
```

### Regression Trees

```{r}

set.seed(1)

tree.onset20=tree(ep_end  ~ newconf+onset2+onset3+onset5+onset10+duration+incompatibility+intensity_level+cumulative_intensity+onset20  , train_valid)
# Only a few of the variables were used in constructing the tree
# lstat: percentage of individuals with lower socioeconomic status
summary(tree.onset20)

# Plot the tree
# Lower values of lstat correspond to more expensive houses
plot(tree.onset20)
text(tree.onset20,pretty=0,cex=0.75)

# cv.tree() to determine whether pruning improves performance
cv.onset20=cv.tree(tree.onset20)
# It doesn't seem to be the case
plot(cv.onset20$size,cv.onset20$dev,type="b")

# prune.tree(): function to prune to be used in case we wanted to prune the tree
prune.onset20=prune.tree(tree.onset20,best=5)
plot(prune.onset20)
text(prune.onset20,pretty=0,cex=0.75)

# Predicting based on CV results (i.e., use the unpruned tree)
yhat=predict(tree.onset20,newdata=test_valid)

# plot(yhat,test_valid$onset20)
# abline(0,1)
# Test error
mse=mean((yhat-test_valid$onset20)^2)
mse
# This model leads to test predictions that are within around $5-6K of the true
# median home value for the suburb
sqrt(mse)
```

### Random Forests

```{r}
# By default randomForest() uses m=p/3 for regression and m=sqrt(p) for classification
# Let's try m=6
set.seed(1)
rf=randomForest(ep_end ~ newconf+onset2+onset3+onset5+onset10+duration+incompatibility+intensity_level+cumulative_intensity+onset20,data=train_valid,mtry=5,importance =T)
yhat.rf = predict(tree.onset20,newdata=test_valid)

mean((yhat.rf-as.integer(test_valid$onset20))^2)

# importance(): view the importance of each variable
# %IncMSE: mean decrease of accuracy in predictions on the OOB samples when a 
# given variable is excluded from the model
# IncNodeImpurity: total decrease in node impurity that results from splits over
# that variable, averaged over all trees (RSS in regr. vs. deviance in class.)
importance(rf)

# varImpPlot(): Variance importance plot
varImpPlot(rf)
```

### Other Models

-   Penalized Logistic Regression -`plr`

-   Conditional Inference Random Forest -`cforest`

-   Random Forest - `rf`

-   Bayesian Generalized Linear Model -`bayesglm`

-   Boosted Generalized Additive Model **-** `gamboost`

-   Support Vector Machines with Linear Kernel - `svmLinear`

```{r}
library(caret)

#specify the cross-validation method
ctrl <- trainControl(method = "cv")

#fit a regression model and use LOOCV to evaluate performance
model <- train(ep_end ~ newconf+onset2+onset3+onset5+onset10+duration+incompatibility+intensity_level+cumulative_intensity+onset20, data = train_better, method = "knn", trControl = ctrl)

#view summary of LOOCV               
print(model)
```

```{r}
predictions <- predict(model, test_better, type="raw")

message('0 for no conflict, 1 for new conflict')
message('Confusion Matrix')

# Confusion Matrix
table(predictions,test_better$ep_end)

# Test Error
message('Test Error Rate')
mean(predictions!=test_better$ep_end)
```

```{r}
confusionMatrix(data = predict(model, test_better), test_better$onset20)
```

# **Phase 3: Predicting War Susceptibility**

### Logistic Regression Model

```{r}
# Making model with all input variables

#There is not enough variation in onset1 so we will not include in the regression
glm.fits = glm(intensity_level ~ newconf+onset2+onset3+onset5+onset10+onset20+duration+year_prev+duration+cumulative_intensity+ep_end+incompatibility,
               data = train_better, family = binomial)

# glm.fits = glm(duration ~ onset2+onset3+onset5+onset10+onset20, data = train_better)

summary(glm.fits)
```

```{r}
# Make predictions based on model
glm.probs = predict(glm.fits,test_better, type="response")

# Initialize vector with 109 elements
glm.pred = rep(0, nrow(test_better))
# Assign 1 to probabilities > 0.5
glm.pred[glm.probs >.5]=1

message('0 for no conflict, 1 for new conflict')
message('Confusion Matrix')

# Confusion Matrix
table(glm.pred,test_better$intensity_level)

# Test Error
message('Test Error Rate')
mean(glm.pred!=test_better$intensity_level)
```

### LDA Model

```{r}

# Making model with all input variables
lda.fit=lda(intensity_level ~ newconf+onset2+onset3+onset5+onset10+onset20+duration+year_prev+duration+cumulative_intensity+ep_end+incompatibility,
               data = train_better)

lda.fit

summary(lda.fit)
lda.pred <- predict(lda.fit , test_better)

message('2 for benign, 4 for malignant')
message('Confusion Matrix')
# Confusion Matrix
lda.class <- lda.pred$class
table(lda.class, test_better$ep_end)

message('Test Error Rate')
# Test Error
mean(lda.class != test_better$ep_end)
```

#### Linear Discriminants

```{r}
plot(lda.fit)
```

### Decision Trees

```{r}
tree.onset20=tree(intensity_level ~ newconf+onset2+onset3+onset5+onset10+onset20+duration+year_prev+duration+cumulative_intensity+ep_end+incompatibility, war_data_valid)
summary(tree.onset20)
plot(tree.onset20)
text(tree.onset20, pretty = 0, cex=0.75)
```

### Decision Trees (With Training & Testing)

```{r}
# Train using training set
tree.onset20=tree(intensity_level ~ newconf+onset2+onset3+onset5+onset10+onset20+duration+year_prev+duration+cumulative_intensity+ep_end+incompatibility , train_better)

# Test on test set using predict()
# type="class" to return the class prediction
tree.pred=predict(tree.onset20,test_better,type="class")

# Confusion matrix
conf.matrix <- table(tree.pred,test_better$intensity_level)
conf.matrix

# Accuracy on test set
(conf.matrix[1,1] + conf.matrix[2,2])/(conf.matrix[1,1] + conf.matrix[2,2] + conf.matrix[1,2]+ conf.matrix[2,1])
```

### Regression Trees

```{r}

set.seed(1)

tree.onset20=tree(intensity_level ~ newconf+onset2+onset3+onset5+onset10+onset20+duration+year_prev+duration+cumulative_intensity+ep_end+incompatibility  , train_better)
# Only a few of the variables were used in constructing the tree
# lstat: percentage of individuals with lower socioeconomic status
summary(tree.onset20)

# Plot the tree
# Lower values of lstat correspond to more expensive houses
plot(tree.onset20)
text(tree.onset20,pretty=0,cex=0.75)

# cv.tree() to determine whether pruning improves performance
cv.onset20=cv.tree(tree.onset20)
# It doesn't seem to be the case
plot(cv.onset20$size,cv.onset20$dev,type="b")

# prune.tree(): function to prune to be used in case we wanted to prune the tree
prune.onset20=prune.tree(tree.onset20,best=5)
plot(prune.onset20)
text(prune.onset20,pretty=0,cex=0.75)

# Predicting based on CV results (i.e., use the unpruned tree)
yhat=predict(tree.onset20,newdata=test_better)

# plot(yhat,test_valid$onset20)
# abline(0,1)
# Test error
mse=mean((yhat-test_valid$onset20)^2)
mse
# This model leads to test predictions that are within around $5-6K of the true
# median home value for the suburb
sqrt(mse)
```

### Random Forests

```{r}
# By default randomForest() uses m=p/3 for regression and m=sqrt(p) for classification
# Let's try m=6
set.seed(1)
rf=randomForest(intensity_level ~ newconf+onset2+onset3+onset5+onset10+onset20+duration+year_prev+duration+cumulative_intensity+ep_end+incompatibility, data=train_better,mtry=5,importance =T)
yhat.rf = predict(tree.onset20,newdata=test_better)

mean((yhat.rf-as.integer(test_better$intensity_level))^2)

# importance(): view the importance of each variable
# %IncMSE: mean decrease of accuracy in predictions on the OOB samples when a 
# given variable is excluded from the model
# IncNodeImpurity: total decrease in node impurity that results from splits over
# that variable, averaged over all trees (RSS in regr. vs. deviance in class.)
importance(rf)

# varImpPlot(): Variance importance plot
varImpPlot(rf)
```

# Other Models

-   Penalized Logistic Regression -`plr`

-   Conditional Inference Random Forest -`cforest`

-   Random Forest - `rf`

-   Bayesian Generalized Linear Model -`bayesglm`

-   Boosted Generalized Additive Model **-** `gamboost`

-   Support Vector Machines with Linear Kernel - `svmLinear`

```{r}
library(caret)

#specify the cross-validation method
ctrl <- trainControl(method = "cv")

#fit a regression model and use LOOCV to evaluate performance
model <- train(intensity_level ~ newconf+onset2+onset3+onset5+onset10+onset20+duration+year_prev+duration+cumulative_intensity+ep_end+incompatibility, data = train_better, method = "knn", trControl = ctrl)

#view summary of LOOCV               
print(model)
```

```{r}
predictions <- predict(model, test_better, type="raw")

message('0 for no conflict, 1 for new conflict')
message('Confusion Matrix')

# Confusion Matrix
table(predictions,test_better$intensity_level)

# Test Error
message('Test Error Rate')
mean(predictions!=test_better$intensity_level)
```

```{r}
confusionMatrix(data = predict(model, test_better), test_better$ep_end)
```
